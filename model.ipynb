{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RealBJr/sign-language-classifier/blob/training-initial-steps/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Environment Setup"
      ],
      "metadata": {
        "id": "v5XMc_91_Cns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "scHs6LDEPFzz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Data"
      ],
      "metadata": {
        "id": "TWppwq7M--h7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1) Create Datasets"
      ],
      "metadata": {
        "id": "RSwNED-5pqX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of dummy dataset to ensure to ensure that pipeline works:"
      ],
      "metadata": {
        "id": "o_yicS9xnUgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyImageDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000, num_classes=10, image_size=(3, 224, 224)):\n",
        "        self.num_samples = num_samples\n",
        "        self.num_classes = num_classes\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = torch.randn(self.image_size)          # fake image\n",
        "        label = torch.randint(0, self.num_classes, (1,)).item()  # fake class\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "okewL6q8nt-4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2) Data Splitting"
      ],
      "metadata": {
        "id": "Z0zwd_4_qPHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dummy data splitting in training and validation steps, used for testing pipeline works:"
      ],
      "metadata": {
        "id": "UXN617aN4LI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_train_dataset = DummyImageDataset(num_samples=500)\n",
        "dummy_val_dataset   = DummyImageDataset(num_samples=100)"
      ],
      "metadata": {
        "id": "DXS1_gr8qdCJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3) Data Loaders"
      ],
      "metadata": {
        "id": "pHEVUY56DIBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dummy data loaders used for testing pipeline works:"
      ],
      "metadata": {
        "id": "G5187bOQp-AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_train_loader = DataLoader(dummy_train_dataset, batch_size=32, shuffle=True)\n",
        "dummy_val_loader   = DataLoader(dummy_val_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "aY1aOz3VqAC0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4) Data Preprocessing"
      ],
      "metadata": {
        "id": "nrGR1QGi_byY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Training Procedure"
      ],
      "metadata": {
        "id": "MvLYjc6P_ehn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1) Load Models"
      ],
      "metadata": {
        "id": "y4J3Y2rCyL1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our project, we are required to compare the performance between fine tuned pre-trained models using transfer learning, and models trained from scratch. Specifically, we will train:\n",
        "\n",
        "- 3 models using Transfer learning\n",
        "- 9 models from scratch\n",
        "\n",
        "Note: Each models will have similar hyperparameters. The 9 different models will come from using **3 different datasets** with **3 different neural network architectures**."
      ],
      "metadata": {
        "id": "9RqwEfov9eRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1) Pre-trained models"
      ],
      "metadata": {
        "id": "yAx_5-0i9DhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With pretrained models, the accuracy will theoretically be better because the model doesn't have to learn certain characteristics(classes) all over again. Pre-trained models are available [here](https://docs.pytorch.org/vision/main/models.html)."
      ],
      "metadata": {
        "id": "0XA8-czr2e8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_resnet_tl = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "model_mobilenet_tl = models.mobilenet_v2(weights=\"IMAGENET1K_V1\")\n",
        "model_vgg_tl = models.vgg16(weights=\"IMAGENET1K_V1\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PkNEZScQPLJi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the most benefits out of pre-trained models, one strategy involves **\"freezing\"** the feature extractor layers. This means that the weight gradients will not be computed for layers defined for general purpose computer vision. The weights will not be updated. This makes it so **the training will be focused on updating the classifier layer** instead of re-learning everything related to image recognition.\n",
        "\n",
        "Note: This code will freeze all network, later on in **3.2) Classifier Head Replacement**, the classifier layer will be unfroze"
      ],
      "metadata": {
        "id": "Zf_v6kYxEDdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model_resnet_tl.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model_mobilenet_tl.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model_vgg_tl.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "-dneOlEKFPcX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Debug and Test"
      ],
      "metadata": {
        "id": "agU95_eM6Keg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model_resnet_tl)\n",
        "# print(model_mobilenet_tl)\n",
        "# print(model_vgg_tl)\n",
        "\n",
        "# Check if all weights and biases (aka parameters) do not require gradients\n",
        "# for name, param in model_resnet_tl.named_parameters():\n",
        "#     print(name, param.requires_grad)"
      ],
      "metadata": {
        "id": "G77P0MDB6M13"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2) Models from scratch"
      ],
      "metadata": {
        "id": "WRyR3BsT9L1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To be trained on Dataset A\n",
        "model_resnet_A = models.resnet18(weights=None)\n",
        "model_mobilenet_A = models.mobilenet_v2(weights=None)\n",
        "model_vgg_A = models.vgg16(weights=None)\n",
        "\n",
        "# To be trained on Dataset B\n",
        "model_resnet_B = models.resnet18(weights=None)\n",
        "model_mobilenet_B = models.mobilenet_v2(weights=None)\n",
        "model_vgg_B = models.vgg16(weights=None)\n",
        "\n",
        "# To be trained on Dataset C\n",
        "model_resnet_C = models.resnet18(weights=None)\n",
        "model_mobilenet_C = models.mobilenet_v2(weights=None)\n",
        "model_vgg_C = models.vgg16(weights=None)"
      ],
      "metadata": {
        "id": "ifRGDnjW9SUL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Debug and Test"
      ],
      "metadata": {
        "id": "jbyC0jeZ9aQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(model_resnet_A)\n",
        "# print(model_mobilenet_A)\n",
        "# print(model_vgg_A)\n",
        "\n",
        "# print(model_resnet_B)\n",
        "# print(model_mobilenet_B)\n",
        "# print(model_vgg_B)\n",
        "\n",
        "# print(model_resnet_C)\n",
        "# print(model_mobilenet_C)\n",
        "# print(model_vgg_C)"
      ],
      "metadata": {
        "id": "OAIS-8jw9cDP"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3) Create a Models Dictionary"
      ],
      "metadata": {
        "id": "_EQXgMIpNkF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_dict = {\n",
        "    # Dataset A — from scratch\n",
        "    \"resnet_A\": models.resnet18(weights=None),\n",
        "    \"mobilenet_A\": models.mobilenet_v2(weights=None),\n",
        "    \"vgg_A\": models.vgg16(weights=None),\n",
        "\n",
        "    # Dataset B — from scratch\n",
        "    \"resnet_B\": models.resnet18(weights=None),\n",
        "    \"mobilenet_B\": models.mobilenet_v2(weights=None),\n",
        "    \"vgg_B\": models.vgg16(weights=None),\n",
        "\n",
        "    # Dataset C — from scratch\n",
        "    \"resnet_C\": models.resnet18(weights=None),\n",
        "    \"mobilenet_C\": models.mobilenet_v2(weights=None),\n",
        "    \"vgg_C\": models.vgg16(weights=None),\n",
        "\n",
        "    # Transfer learning models\n",
        "    \"resnet_tl\": models.resnet18(weights=\"IMAGENET1K_V1\"),\n",
        "    \"mobilenet_tl\": models.mobilenet_v2(weights=\"IMAGENET1K_V1\"),\n",
        "    \"vgg_tl\": models.vgg16(weights=\"IMAGENET1K_V1\"),\n",
        "}"
      ],
      "metadata": {
        "id": "FNKnpZM1N3R3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2) Classifier Head Replacement"
      ],
      "metadata": {
        "id": "QSHnxxPoADQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing the classifier head means that we are tailoring the last layer(s) of our neural network to the amount of categories that said classifier has to predict"
      ],
      "metadata": {
        "id": "F4jL5L8ewQY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Util function to replace classifier(aka head) of model\n",
        "def replace_classifier(model, num_classes):\n",
        "    if hasattr(model, \"fc\"):  # ResNet\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    elif hasattr(model, \"classifier\"):\n",
        "        # MobileNetV2 or VGG16\n",
        "        if isinstance(model.classifier, nn.Sequential):\n",
        "            last_layer = model.classifier[-1]\n",
        "            model.classifier[-1] = nn.Linear(last_layer.in_features, num_classes)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown classifier structure\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model architecture\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "B5EOKxNYwO8s"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 26\n",
        "\n",
        "for name, model in models_dict.items():\n",
        "    models_dict[name] = replace_classifier(model, num_classes)"
      ],
      "metadata": {
        "id": "X3yJ4y9HMvPK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug and Test"
      ],
      "metadata": {
        "id": "_Ty7UVVA5WkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# images, labels = next(iter(dummy_train_loader))\n",
        "\n",
        "# print(\"--- Output Shape Check ---\")\n",
        "# with torch.no_grad():\n",
        "#     for name, model in models_dict.items():\n",
        "#         model.eval()\n",
        "#         outputs = model(images)\n",
        "#         print(f\"{name}: {outputs.shape}\")\n",
        "#         del outputs"
      ],
      "metadata": {
        "id": "hq8m5gxm5hKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3) Optimizer + Loss Function Setup"
      ],
      "metadata": {
        "id": "uBGTZ3UdA27y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimizer is the algorithm uused to update the learning parameters. When giving them to the optimizer, we need to ensure that we give the unfroze parameters, those that can be updated."
      ],
      "metadata": {
        "id": "dTnSYgv5d7cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizers for our models\n",
        "optimizers = {}\n",
        "for name, model in models_dict.items():\n",
        "    optimizers[name] = optim.AdamW(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=0.001,\n",
        "        weight_decay=1e-4\n",
        "    )"
      ],
      "metadata": {
        "id": "-Gcxjj0DdEeg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug and Test"
      ],
      "metadata": {
        "id": "icZjNKa8fduO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# images, labels = next(iter(dummy_train_loader))\n",
        "\n",
        "# print(\"--- Loss Sanity Check ---\")\n",
        "# with torch.no_grad():\n",
        "#     for name, model in models_dict.items():\n",
        "#         model.eval() # Set to evaluation mode\n",
        "#         outputs = model(images)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         print(f\"{name}: loss = {loss.item():.4f}\")\n",
        "#         # Clear large objects from memory\n",
        "#         del outputs\n",
        "\n",
        "# print(\"\\n--- Optimizers Loaded ---\")\n",
        "# print(f\"Total optimizers: {len(optimizers)}\")"
      ],
      "metadata": {
        "id": "9b62_7DTfhMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4) Forward Pass and Loss Calculation"
      ],
      "metadata": {
        "id": "oZuxCB36BCQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is meant to be used within the training loop to calculate the loss for given model"
      ],
      "metadata": {
        "id": "0fG2br7mobSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_loss(model, images, labels):\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    return loss, outputs"
      ],
      "metadata": {
        "id": "As-s_GcojUj2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Debug and Test"
      ],
      "metadata": {
        "id": "3dmgDETMnEa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_images, test_labels = next(iter(dummy_train_loader))\n",
        "test_model = models_dict['resnet_A']\n",
        "test_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    loss_value, outputs_value = calculate_loss(test_model, test_images, test_labels)\n",
        "\n",
        "print(f'Test Loss: {loss_value.item():.4f}')\n",
        "print(f'Output Shape: {outputs_value.shape}')"
      ],
      "metadata": {
        "id": "ptc1r5bjndZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5) Backpropagation"
      ],
      "metadata": {
        "id": "gHv38RDzBLVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6) Weight Update Step"
      ],
      "metadata": {
        "id": "GVodstcHBRwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.7) Validation Phase"
      ],
      "metadata": {
        "id": "CFapdsTNBtqK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.8) Optimization"
      ],
      "metadata": {
        "id": "4oAse0ZFBWef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.9) Train and Save Models"
      ],
      "metadata": {
        "id": "Vx35lU_7CjEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Model Evaluation & Analysis"
      ],
      "metadata": {
        "id": "9IeFUmE2DwBW"
      }
    }
  ]
}